<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Deep Chain HDRI: Reconstructing a High Dynamic Range Image from a Single Low Dynamic Range Image</title>
	<meta property="og:image" content="./resources/main_dataset.gif"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Deep Chain HDRI" />
	<meta property="og:description" content="Reconstructing a High Dynamic Range Image from a Single Low Dynamic Range Image" />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Deep Chain HDRI: Reconstructing a High Dynamic Range Image from a Single Low Dynamic Range Image</span>
		<table align=center width=600px>
			<table align=center width=600px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.com/citations?user=iGSaIU0AAAAJ">Siyeong Lee</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://ieeexplore.ieee.org/author/37086386559">Gwon Hwan An</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://ieeexplore.ieee.org/author/37085778588">Suk-ju Kang</a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://ieeexplore.ieee.org/document/8457442'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href=''>[Dataset]</a></span>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:500px" src="./resources/main_structure.gif"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					Proposed deep chain HDRI architecture. Given an LDR image with a middle exposure value (EV 0), the EV ±1, ±2, ±3 images are inferred sequentially through the network. 
					When the EV of the inferred image is far from the middle exposure value, the structure depth goes deeper than that of the image that has less exposure difference to infer the mapping relation more accurately. 
					After finishing the process through the proposed network, a total of six LDR images are inferred to generate the LDR image stack. 
					Using an HDRI synthesis technique (e.g. the method of Debevec and Malik, etc.), an HDR image is generated from the pseudo multi exposure stack.
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Recently, high dynamic range (HDR) imaging has attracted much attention as a technology to reflect human visual characteristics owing to the development of the display and camera technology. This paper proposes a novel deep neural network model that reconstructs an HDR image from a single low dynamic range (LDR) image. The proposed model is based on a convolutional neural network composed of dilated convolutional layers and infers LDR images with various exposures and illumination from a single LDR image of the same scene. Then, the final HDR image can be formed by merging these inference results. It is relatively simple for the proposed method to find the mapping between the LDR and an HDR with a different bit depth because of the chaining structure inferring the relationship between the LDR images with brighter (or darker) exposures from a given LDR image. The method not only extends the range but also has the advantage of restoring the light information of the actual physical world. The proposed method is an end-to-end reconstruction process, and it has the advantage of being able to easily combine a network to extend an additional range. In the experimental results, the proposed method shows quantitative and qualitative improvement in performance, compared with the conventional algorithms.			</td>
		</tr>
	</table>
	<br>

	<hr>

	<center><h1>Dataset</h1></center>
	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:450px" src="./resources/main_dataset.gif"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<ul>
						<li> VDS dataset: the dataset contains 96 scenes that cover a wide variety of content, e.g., natural scenes (both indoor and outdoor), wooded grounds, buildings, etc. </i>
						<li> This paper describes the dataset and the methodology followed when collecting it in much greater detail. 
							Please cite it if you intend to use this dataset.</li>
						<li> All ground truth HDR images are synthesized by 
							the <a href="https://github.com/banterle/HDR_Toolbox">HDR Toolbox</a> implementation 
							of <a href="https://dl.acm.org/doi/10.1145/258734.258884">Debevec and Malik 1997</a>. 
							And, all tone-mapped images are generated by the <a href="https://github.com/banterle/HDR_Toolbox">HDR Toolbox</a> implementation 
							of <a href="https://dl.acm.org/doi/10.1145/566654.566575">Reinhard et al. 2002</a>. </li> 
					</ul>
				</td>
			</tr>
		</center>
	</table>
	<br>
	<hr>
	<table align=center width=450px>
		<center><h1>Paper</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">S. Lee, G.H. An, S-J. Kang.<br>
				<b>Deep Chain HDRI: Reconstructing a High Dynamic Range Image from a Single Low Dynamic Range Image</b><br>
				IEEE Access, 2018.<br>
				(hosted on <a href="https://ieeexplore.ieee.org/document/8457442">IEEE Access</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>
	<hr>
	<br>
	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Future works</h1></center>
						<b>Inverse tone mapping:</b></br>
						<ul>
							<li>Siyeong Lee, Gwon Hwan An, and Suk-Ju Kang. 
							"Deep recursive hdri: Inverse tone mapping using generative adversarial networks." 
							proceedings of the European Conference on Computer Vision (ECCV). 2018.
						 	 <a href='https://openaccess.thecvf.com/content_ECCV_2018/papers/Siyeong_Lee_Deep_Recursive_HDRI_ECCV_2018_paper.pdf'>[Paper]</a>,
						 	 <a href='https://github.com/Siyeong-Lee/Deep_Recursive_HDRI'>[Code]</a> </li>
							<li> Siyeong Lee, So Yeon Jo, Gwon Hwan An, and Suk-Ju Kang.
							"Learning to Generate Multi-Exposure Stacks with Cycle Consistency for High Dynamic Range Imaging." 
							IEEE Transactions on Multimedia (2020). </li>
							<li> Jung Hee Kim*, Siyeong Lee*, and Suk-Ju Kang. 
							End-to-End Differentiable Learning to HDR Image Synthesis for Multi-exposure Images." 
							Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). 2021.
							  <a href='https://arxiv.org/abs/2006.15833'>[Paper]</a>,
							  <a href='https://github.com/JungHeeKim29/DiffHDRsyn'>[Code]</a> </li>
							<li> So Yeon Jo, Siyeong Lee, and Namhyun Ahn, Suk-Ju Kang.
							"Deep Arbitrary HDRI: Inverse Tone Mapping with Controllable Exposure Changes." 
							IEEE Transactions on Multimedia (2021). </li>
						</ul>
						<br>
						<b>Tone mapping:</b>
						<ul>
							<li>Gwon Hwan An, Siyeong Lee, Yong-Deok Ahn, and Suk-Ju Kang. 
							"Deep Tone‐mapped HDRNET for High Dynamic Range Image Restoration." 
							SID Symposium Digest of Technical Papers. (2018).</li>
						</ul>
						</td>				
				</left>
			</td>
		</tr>
	</table>

	<hr>

	<br>
	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					<ul>
						<li> We are especially grateful to Junghee Kim for helping us reorganize the dataset for release.</li>
						<li>This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>. </li>
					</ul>
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>
